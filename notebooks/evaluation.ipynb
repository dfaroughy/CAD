{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup_path  # adds project root to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77235ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-3.0.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/dario/anaconda3/envs/CAD/lib/python3.12/site-packages (from pandas) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dario/anaconda3/envs/CAD/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dario/anaconda3/envs/CAD/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-3.0.1-cp312-cp312-macosx_10_13_x86_64.whl (10.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "Successfully installed pandas-3.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m RESULTS_DIR = Path(\u001b[33m'\u001b[39m\u001b[33m../Analysis/results\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m MODELS = [\u001b[33m'\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mclaude-3.5-sonnet\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgemma-3-27b-it\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mqwen2.5-vl-72b-instruct\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_DIR = Path('../Analysis/results')\n",
    "MODELS = ['gpt-4o', 'claude-3.5-sonnet', 'gemma-3-27b-it', 'qwen2.5-vl-72b-instruct']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-load",
   "metadata": {},
   "source": [
    "## Load per-model CSVs\n",
    "Run `compare_generated_to_truth.py --all-models` first to populate `Analysis/results/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for model in MODELS:\n",
    "    csv_path = RESULTS_DIR / f'compare_{model}.csv'\n",
    "    if csv_path.exists():\n",
    "        dfs[model] = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        print(f'Missing: {csv_path}')\n",
    "\n",
    "print(f'Loaded {len(dfs)} model(s):', list(dfs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-leaderboard",
   "metadata": {},
   "source": [
    "## Leaderboard\n",
    "Metrics (lower is better for vol_rel / hausdorff / xi_l2; higher p_value = more similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leaderboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE_METRICS = ['vol_rel', 'hausdorff', 'xi_l2', 'p_value']\n",
    "\n",
    "rows = []\n",
    "for model, df in dfs.items():\n",
    "    ok   = df[df['status'] == 'ok']\n",
    "    n    = len(df)\n",
    "    code_pass = (df['code_success'] == True).sum() / n if n > 0 else float('nan')\n",
    "    shape_pass = len(ok) / n if n > 0 else float('nan')\n",
    "    row = {'model': model, 'n_total': n,\n",
    "           'code_pass_rate': round(code_pass, 3),\n",
    "           'shape_eval_rate': round(shape_pass, 3)}\n",
    "    for m in SHAPE_METRICS:\n",
    "        vals = ok[m].dropna()\n",
    "        row[f'{m}_mean']   = round(vals.mean(), 4)   if len(vals) else float('nan')\n",
    "        row[f'{m}_median'] = round(vals.median(), 4) if len(vals) else float('nan')\n",
    "    rows.append(row)\n",
    "\n",
    "leaderboard = pd.DataFrame(rows).set_index('model')\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-boxplots",
   "metadata": {},
   "source": [
    "## Metric distributions per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxplots",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_labels = {\n",
    "    'vol_rel':   'Volume rel. diff  (↓ better)',\n",
    "    'hausdorff': 'Hausdorff dist    (↓ better)',\n",
    "    'xi_l2':     'ξ(r) L2           (↓ better)',\n",
    "    'p_value':   'p-value (Fisher)  (↑ better)',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "model_names = list(dfs.keys())\n",
    "short_names = [m.split('-')[0] + '-' + m.split('-')[1] if '-' in m else m for m in model_names]\n",
    "\n",
    "for ax, metric in zip(axes, SHAPE_METRICS):\n",
    "    data = [dfs[m].query(\"status == 'ok'\")[metric].dropna().values for m in model_names]\n",
    "    bp = ax.boxplot(data, patch_artist=True, medianprops=dict(color='black', linewidth=2))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 0.6, len(model_names)))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    ax.set_title(metric_labels[metric], fontsize=10)\n",
    "    ax.set_xticks(range(1, len(model_names) + 1))\n",
    "    ax.set_xticklabels(short_names, rotation=20, ha='right', fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Shape Metric Distributions by Model', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-passrate",
   "metadata": {},
   "source": [
    "## Code pass rate vs shape quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passrate-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "shape_metrics_scatter = ['hausdorff', 'xi_l2', 'p_value']\n",
    "colors = plt.cm.tab10(np.linspace(0, 0.6, len(model_names)))\n",
    "\n",
    "for ax, metric in zip(axes, shape_metrics_scatter):\n",
    "    for color, model in zip(colors, model_names):\n",
    "        df = dfs[model]\n",
    "        code_pass = (df['code_success'] == True).sum() / len(df)\n",
    "        ok_vals   = df.query(\"status == 'ok'\")[metric].dropna()\n",
    "        if len(ok_vals) == 0:\n",
    "            continue\n",
    "        shape_score = ok_vals.median()\n",
    "        ax.scatter(code_pass, shape_score, color=color, s=120, zorder=3,\n",
    "                   label=model.split('-')[0])\n",
    "        ax.annotate(model.split('-')[0], (code_pass, shape_score),\n",
    "                    textcoords='offset points', xytext=(6, 4), fontsize=8)\n",
    "    ax.set_xlabel('Code pass rate', fontsize=10)\n",
    "    ax.set_ylabel(f'Median {metric}', fontsize=10)\n",
    "    ax.set_title(f'Pass rate vs {metric}', fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Code Executability vs Shape Quality', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-errors",
   "metadata": {},
   "source": [
    "## Error breakdown per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import code_error_bins\n",
    "import os\n",
    "\n",
    "GEN_ROOT = Path('../generated')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(dfs), figsize=(5 * len(dfs), 4))\n",
    "if len(dfs) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, model in zip(axes, model_names):\n",
    "    gen_dir = GEN_ROOT / model\n",
    "    if not gen_dir.exists():\n",
    "        ax.set_title(f'{model}\\n(dir missing)')\n",
    "        continue\n",
    "    bins = code_error_bins(str(gen_dir))\n",
    "    labels = list(bins.keys())\n",
    "    fracs  = [bins[k]['fraction'] for k in labels]\n",
    "    bar_colors = ['#2ecc71' if k == 'passed' else '#e74c3c' for k in labels]\n",
    "    bars = ax.bar(labels, fracs, color=bar_colors, edgecolor='white', linewidth=0.5)\n",
    "    for bar, frac in zip(bars, fracs):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                f'{frac:.0%}', ha='center', va='bottom', fontsize=9)\n",
    "    ax.set_ylim(0, 1.12)\n",
    "    ax.set_title(model, fontsize=9)\n",
    "    ax.set_ylabel('Fraction of files')\n",
    "    ax.set_xticklabels(labels, rotation=25, ha='right', fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Code Error Breakdown by Model', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-pairwise",
   "metadata": {},
   "source": [
    "## Per-sample inspection\n",
    "Pick a model and look at the worst / best shapes by hausdorff distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pairwise",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o'   # change as needed\n",
    "\n",
    "df = dfs[MODEL].query(\"status == 'ok'\").copy()\n",
    "df = df.sort_values('hausdorff')\n",
    "\n",
    "print(f'=== {MODEL}: best 5 by Hausdorff ===')\n",
    "print(df[['sample_id', 'vol_rel', 'hausdorff', 'xi_l2', 'p_value']].head(5).to_string(index=False))\n",
    "\n",
    "print(f'\\n=== {MODEL}: worst 5 by Hausdorff ===')\n",
    "print(df[['sample_id', 'vol_rel', 'hausdorff', 'xi_l2', 'p_value']].tail(5).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
